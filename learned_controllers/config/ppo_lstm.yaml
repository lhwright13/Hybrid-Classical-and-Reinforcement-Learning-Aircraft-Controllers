# PPO + LSTM Training Configuration for Rate Controller

# Environment settings
environment:
  difficulty: "easy"  # Start with easy, progress to medium/hard
  episode_length: 10.0  # seconds
  dt: 0.02  # 50 Hz control rate
  command_type: "step"  # step, ramp, sine, random

# Training settings
training:
  total_timesteps: 1000000  # 1M steps
  n_envs: 4  # Parallel environments
  eval_freq: 10000  # Evaluate every N steps
  save_freq: 50000  # Save model every N steps
  log_interval: 10  # Log every N episodes

# Curriculum learning phases
curriculum:
  enabled: true
  phases:
    - name: "easy"
      difficulty: "easy"
      timesteps: 300000
      command_type: "step"

    - name: "medium"
      difficulty: "medium"
      timesteps: 400000
      command_type: "step"

    - name: "hard"
      difficulty: "hard"
      timesteps: 300000
      command_type: "random"

# PPO Hyperparameters
ppo:
  learning_rate: 3.0e-4
  n_steps: 2048  # Steps per environment per update
  batch_size: 64
  n_epochs: 10
  gamma: 0.99  # Discount factor
  gae_lambda: 0.95  # GAE parameter
  clip_range: 0.2
  clip_range_vf: null  # No value function clipping
  ent_coef: 0.01  # Entropy coefficient (exploration)
  vf_coef: 0.5  # Value function coefficient
  max_grad_norm: 0.5
  use_sde: false  # State-dependent exploration
  sde_sample_freq: -1

# LSTM Architecture
lstm:
  enabled: true
  lstm_hidden_size: 256
  n_lstm_layers: 2
  features_dim: 128

# Alternative: MLP Architecture (if lstm.enabled = false)
mlp:
  net_arch: [256, 128, 64]

# Normalization
normalize:
  obs: false  # Don't normalize observations (already scaled)
  reward: false  # Don't normalize rewards

# Model paths
paths:
  model_save_dir: "learned_controllers/models/checkpoints"
  tensorboard_log: "learned_controllers/logs/tensorboard"
  best_model_path: "learned_controllers/models/best_rate_controller"

# Evaluation
evaluation:
  n_eval_episodes: 10
  deterministic: true
  render: false

# Random seed
seed: 42
